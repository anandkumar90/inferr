---
title: "Introduction to inferr"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to inferr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo=FALSE, message=FALSE}
library(inferr)
library(dplyr)
```

Inferential statistics allows us to make generalizations about populations using data drawn from the population.
We use them when it is impractical or impossible to collect data about the whole population under study and instead,
we have a sample that represents the population under study and using inferential statistics technique, we make
generalizations about the population from the sample. The **stats** package provides a solid set of statistical tests but leaves enough flexibility to add more details in terms of data input options, expanding and structuring the test results.

The **inferr** package:

- builds upon the statistical tests provided in **stats**  
- provides additional and flexible input options
- more detailed and structured test results

As of version 0.1, **inferr** includes a select set of parametric and non-parametric statistical tests which are listed below:

- 1 Sample t Test
- Paired Sample t Test
- Independent Sample t Test
- 1 Sample Proportion Test
- 2 Sample Proportion Test
- 1 Sample Variance Test
- 2 Sample Variance Test
- Binomial Test
- ANOVA
- Chi Square Goodness of Fit Test
- Chi Square Independence Test
- Levene's Test
- Cochran's Q Test
- McNemar Test
- Runs Test for Randomness

These tests are described in more detail in the following sections.

## One Sample t Test

A one sample t-test is used to determine whether a sample of comes from a population with a specific mean. The observations must be continuous, independent of each other, approximately distributed and should not contain any outliers.

### Example

Using the hsb data, test whether the average of write differs significantly from 50.

```{r ttest}
ttest(mtcars$mpg, mu = 50, type = 'all')
```

## Paired t test

A paired (samples) t-test is used when you want to compare the means between two related groups of observations on some continuous dependent variable. In a paired sample test, each subject or entity is measured twice. It can be used to evaluate the effectiveness of training programs or treatments. If the dependent variable is dichotomous, use the McNemar test.

### Examples

Using the hsb data, test whether the mean of read is equal to the mean of write.

```{r pair1}
# Lower Tail Test
paired_ttest(hsb$read, hsb$write, alternative = 'less')

# Test all alternatives
paired_ttest(hsb$read, hsb$write, alternative = 'all')
```

## Two Independent Sample t Test
An independent samples t-test is used to compare the means of a normally distributed continuous dependent variable for two unrelated groups. The dependent variable must be approximately normally distributed and the cases/subjects in the two groups  must be different i.e. a subject in one group cannot also be a subject of the other group. It can be used to answer whether:

- average number of products produced by two machines differ significantly?
- average salaries of graduate students differ based on gender?

### Example

Using the hsb2 data , test whether the mean for write is the same for males and females.

```{r ind}
ind_ttest(mtcars, 'am', 'mpg', alternative = 'all')
```

## One Sample Test of Proportion

One sample test of proportion compares proportion in one group to a specified population proportion.

### Examples

Using hsb data, test whether the proportion of females is 50%.

```{r os_prop1}
# Using Variables
prop_test(as.factor(hsb$female), prob = 0.5)

# Calculator
prop_test(200, prob = 0.5, phat = 0.3)
```

## Two Sample Test of Proportion

Two sample test of proportion performs tests on the equality of proportions using large-sample statistics. It tests that a categorical variable has the same proportion within two groups or that two variables have the same proportion.

### Examples

```{r ts_prop1}
# Using Variables
ts_prop_test(var1 = mtcars$am, var2 = mtcars$vs, alternative = 'all')

# Using Grouping Variable
ts_prop_grp(var = mtcars$am, group = mtcars$vs, alternative = 'all')

# Calculator
ts_prop_calc(n1 = 30, n2 = 25, p1 = 0.3, p2 = 0.5, alternative = 'all')
```

## One Sample Variance Test

One sample variance comparison test compares the standard deviation (variances) to a hypothesized value. It determines whether the standard deviation of a population is equal to a hypothesized value. It can be used to answer the following questions:

- Is the variance equal to some pre-determined threshold value?
- Is the variance greater than some pre-determined threshold value?
- Is the variance less than some pre-determined threshold value?

### Examples

Using the mtcars data, compare the standard deviation of mpg to a hypothesized value.

```{r os_var}
# Lower Tail Test
os_vartest(mtcars$mpg, 0.3, alternative = 'less')

# Test all alternatives
os_vartest(mtcars$mpg, 0.3, alternative = 'all')
```

## Two Sample Variance Test

Two sample variance comparison tests equality of standard deviations (variances). It tests that the standard deviation of a continuous variable is same within two groups or the standard deviation of two continuous variables is equal.

### Example

```{r ts_var1}
# Using Grouping Variable
var_test(mtcars$mpg, group_var = mtcars$vs, alternative = 'all')

# Using Variables
var_test(mtcars$mpg, mtcars$qsec, alternative = 'all')
```

## Binomial Probability Test

A one sample binomial test allows us to test whether the proportion of successes on a two-level categorical dependent variable significantly differs from a hypothesized value.

### Examples

Using the hsb data, test whether the proportion of females and males are equal.

```{r binom_calc}
# Using variables
binom_test(as.factor(hsb$female), prob = 0.5)

# Using calculator
binom_calc(32, 16, prob = 0.5)
```

## ANOVA

The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of two or more independent (unrelated) groups. It tests the null hypothesis that samples in two or more groups are drawn from populations with the same mean values. It cannot tell you which specific groups were statistically significantly different from each other but only that at least two groups were different and can be used only for numerical data.

### Examples

Using the hsb data, test whether the mean of write differs between the three program types.

```{r anova}
owanova(hsb, 'write', 'prog')
```

## Chi Square Goodness of Fit Test
A chi-square goodness of fit test allows us to compare the observed sample distribution with expected probability distribution.
It tests whether the observed proportions for a categorical variable differ from hypothesized proportions. The proportion of cases expected in each group of categorical variable may be equal or unequal. It can be applied to any univariate distribution for which you can calculate the cumulative distribution function. It is applied to binned data and the value of the chi square test depends on how the data is binned. For the chi square approximation to be valid, the sample size must be sufficiently large.

### Example

Using the hsb data, test whether the observed proportions for race differs significantly from the
hypothesized proportions.

```{r gof1}
# basic example
chisq_gof(hsb$race, c(20, 20, 20 , 140))

# using continuity correction
chisq_gof(hsb$race, c(20, 20, 20 , 140), correct = TRUE)
```

## Chi Square Test of Independence

A chi-square test is used when you want to test if there is a significant relationship between two nominal (categorical) variables.     

### Examples

Using the hsb data, test if there is a relationship between the type of school attended (schtyp) and students' gender (female).

```{r chi1}
chisq_test(as.factor(hsb$female), as.factor(hsb$schtyp))
```

## Levene's Test

Levene's test is used to determine if k samples have equal variances. It is less sensitive to departures from normality and is an alternative to Bartlett's test. This test returns Levene's robust test statistic and the two statistics proposed by Brown and Forsythe that replace the mean in Levene's formula with alternative location estimators. The first alternative replaces the mean with the median and the second alternative replaces the mean with the 10% trimmed mean.

### Examples

```{r lev1}
# Using Grouping Variable
levene_test(mtcars$mpg, group_var = mtcars$vs)

# Using Variables
levene_test(mtcars$mpg, mtcars$qsec)

# Using Linear Regression Model
m <- lm(mpg ~ vs, data = mtcars)
levene_test(m)

# Using Formula
levene_test(as.formula(paste0('mpg ~ vs')), mtcars)
```

## Cochran's Q Test
Cochran's Q test is an extension to the McNemar test for related samples that provides a method for testing for differences between three or more matched sets of frequencies or proportions. It is a procedure for testing if the proportions of 3 or more dichotomous variables are equal in some population. These outcome variables have been measured on the same people or other statistical units.

### Example

The exam data set contains scores of 15 students for three exams (exam1, exam2, exam3). Test if three exams are equally difficult.

```{r cochran}
cochran_test(exam)
```

## McNemar Test

McNemar test is a non parametric test created by Quinn McNemar and first published in **Psychometrika** in 1947.
It is similar to a paired t test but applied to a dichotomous dependent variable. It is used to test if a statistically
significant change in proportions have occurred on a dichotomous trait at two time points on the same population. It can
be used to answer whether:

- two products are equally appealing?
- proportion of success vs failure significantly change after treatment?
- proportion of voters significantly change prior to and following a significant political development?

### Examples

```{r mc1}
# http://www.ats.ucla.edu/stat/sas/whatstat/whatstat.htm#Mcnemar
mcnemar_test(matrix(c(172, 7, 6, 15), nrow = 2))

# http://www.ats.ucla.edu/stat/stata/whatstat/whatstat.htm#Mcnemar
mcnemar_test(matrix(c(15, 7, 6, 172), nrow = 2))

# http://www.ats.ucla.edu/stat/spss/whatstat/whatstat.htm#Mcnemar
himath <- ifelse(hsb$math > 60, 1, 0)
hiread <- ifelse(hsb$read > 60, 1, 0)
mcnemar_test(table(himath, hiread))
```


## Runs Test for Randomness

Runs Test can be used to decide if a data set is from a random process. It tests whether observations of a sequence are serially independent i.e. whether they occur in a random order by counting how many runs there are above and below a threshold. A run is defined as a series of increasing values or a series of decreasing values. The number of increasing, or decreasing, values is the length of the run.  By default, the median is used as the threshold.  A small number of runs indicates positive serial correlation; a large number indicates negative serial correlation.

### Examples

We will use runs test to check regression residuals for serial correlation.

```{r runs1}
# linear regression
reg <- lm(mpg ~ disp, data = mtcars)

# basic example
runs_test(residuals(reg))

# drop values equal to threshold
runs_test(residuals(reg), drop = TRUE)

# recode data in binary format
runs_test(residuals(reg), split = TRUE)

# use mean as threshold
runs_test(residuals(reg), mean = TRUE)

# threshold to be used for counting runs
runs_test(residuals(reg), threshold = 0)
```

## Credits

The examples and the data set used in the vignette are borrowed from the below listed sources:

- What statistical analysis should I use?  UCLA: Statistical Consulting Group.
from http://www.ats.ucla.edu/stat/mult_pkg/whatstat/
